{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 2: Data Preparation for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will prepare the IMDB movie review sentiment dataset. We will prepare it to fit a model that will predict wether a new reviews has positive or negative sentiment. These sentences are movie reviews and the label reflects whether it is a 'positive' review or a 'negative' review. **Start by downloading the IMDB_Dataset into a pandas DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Download the dataset into a Pandas DataFrame and display the first 5 rows\n",
    "## YOUR CODE HERE\n",
    "imdb = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to process the data first, so that we have fixed length sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We want to split the dataset into classes and labels. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_x = \n",
    "imdb_y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paste the toBinary functon created in HW 1\n",
    "def toBinary(data, positive):\n",
    "    ## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the toBinary method to tranform the sentiment column into binary, 1 for positive and 0 for negative.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toBinary(imdb_y, 'positive')\n",
    "imdb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Lemmatization is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\"\n",
    "    https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "\n",
    "Lemmatize the sentences using any library from the article. Make sure to filter out non-alphabetical characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "\n",
    "def Lemmatize(data):\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    data = data.str.split(\" |,|<br /><br />|\\\"|\\'|!\") # splits the data from sentences to words, feel free to change\n",
    "    lemmatizer = # Initialize lemmatizer\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return data\n",
    "\n",
    "Lemmatize(imdb_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be put into integer form, so each integer represents a unique word, 0 represents a PAD character, 1 represents a START character and 2 represents a character that is unknown because it is not in the top `num_words`. \n",
    "Thus 3 represents the first real word. \n",
    "\n",
    "Also the words should be in decreasing order of frequency, so the word that 3 represents is the most common word in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete CreateDict which will take in a data column <br>\n",
    "    1) Create a Dict that maps {Word: Apperances in dataset} <br>\n",
    "    2) Choose the top N most recurring words and give ascending indexes starting at 3 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numWords = 1000\n",
    "\n",
    "def CreateDict(data, topN):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "    return dict(wordCounter)\n",
    "    \n",
    "wordCounter = CreateDict(imdb_x, numWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete  replaceByIndex which will replace known words with their index and unknown words with a 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replaceByIndex(data, wordCounter):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass \n",
    "\n",
    "imdb_x = replaceByIndex(imdb_x, wordCounter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We want to process the data into NumPy arrays of sequences that are all length 200. If a given sequence is shorter than 200 tokens we want to pad the rest of the sequence out with zeros so that the sequence is 200 long. If the sequence is longer than 200 we want to cut it down to length 200. We also want to add a 1 at the beggining of every sentence (after 0s) to signal the beggining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    \n",
    "    #YOUR CODE HERE \n",
    "    \n",
    "    return processed\n",
    "\n",
    "imdb_x = process_data(imdb_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** separate the dataset into train and test sets, test set should be 1/3 of the set. ** <p>\n",
    "This sklearn method will make your life much easier: \n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "x_train_proc, x_test_proc, y_train, y_test = #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point **your job is done!!!** Congratulations, if done correctly, the sentences are processed and ready to be used as features and labels to train a Recurrent Neural Network (LSTM). You will learn how to do this yourself in the next couple weeks. For now, you can just sit back and \"follow along\" as we build this model using Keras and then train it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will do is initialize the model using Sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Sequential\n",
    "\n",
    "imdb_model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to add an embedding layer. The purpose of an embedding layer is to take a sequence of integers representing words in our case and turn each integer into a dense vector in some embedding space. (This is essentially the idea of Word2Vec). We want to create an embedding layer with vocab size equal to the max num words we allowed when we loaded the data (in this case 1000), and a fixed dense vector of size 32. Then we have to specify the max length of our sequences and we want to mask out zeros in our sequence since we used zero to pad.\n",
    "Use the docs for embedding layer to fill out the missing entries: https://keras.io/layers/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "imdb_model.add(Embedding(2000, 32, input_length=200, mask_zero=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(a)** We add an LSTM layer with 32 outputs, then a Dense layer with 16 neurons, then a relu activation, then a dense layer with 1 neuron, then a sigmoid activation. Then we print out the model summary. The Keras documentation is here: https://keras.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Activation\n",
    "imdb_model.add(LSTM(32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_model.add(Dense(units=16, activation='relu'))\n",
    "imdb_model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(b)** Now we compile the model with binary cross entropy, and the adam optimizer. We include accuracy as a metric in the compile. Then train the model on the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imdb_model.compile(loss=keras.losses.binary_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_model.fit(x_train_proc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", imdb_model.evaluate(x_test_proc, y_test)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you did the data pre-processing correctly you should be getting around an 80% accuracy. congratulations, that is much better than random! If you are getting a test accuracy that is significantly lower, you probably did something wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can look at our predictions and the sentences they correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = imdb_model.predict(x_test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id = wordCounter\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items() if value < 2000}\n",
    "def get_words(token_sequence):\n",
    "    return ' '.join(id_to_word[token] for token in token_sequence)\n",
    "\n",
    "def get_sentiment(y_pred, index):\n",
    "    return 'Positive' if y_pred[index] else 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = [i for i in y_test]\n",
    "y_pred = np.vectorize(lambda x: int(x >= 0.5))(y_pred)\n",
    "correct = []\n",
    "incorrect = []\n",
    "for i, pred in enumerate(y_pred):\n",
    "    if y_test[i] == pred:\n",
    "        correct.append(i)\n",
    "    else:\n",
    "        incorrect.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we print out one of the sequences we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_sentiment(y_pred, correct[10]))\n",
    "print(get_words(x_test_proc[correct[10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And one we got wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_sentiment(y_pred, incorrect[10]))\n",
    "print(get_words(x_test_proc[incorrect[10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see the amount of UNKNOWN characters in the sequence cause by having only 1000 vocab words is hurting our performance. If you want, go back and increase the number of vocab words to 2000 and compare your accuracy. (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Exploration\n",
    "#### Another interesting thing to do is see if our learned embeddings mean anything reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function takes a list of token sequences as inputs \n",
    "# and outputs the corresponding vector outputs of our `Embedding` layer\n",
    "embedding_func = K.function([imdb_model.inputs[0]], [imdb_model.layers[0].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function outputs the embedding of a given word using above function\n",
    "def word_to_embedding(word):\n",
    "    token = word_to_id[word]\n",
    "    seq = [token]\n",
    "    sequences = [seq]\n",
    "    inputs = [process_data(sequences)]\n",
    "    embedding = embedding_func(inputs)\n",
    "    return embedding[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_words = [word for word, token in word_to_id.items() if token < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_word_embeddings = {word: word_to_embedding(word) for word in valid_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used an embedding layer with an output size of 32, our embeddings are going to be 32-dimensional vectors. Humans can't effectively visualize beyond 3 (maybe 4) dimensions so we want to use a dimensionality reduction technique to make our embeddings more visualizable. One such technique is Principal Component Analysis or PCA. The library scikit-learn provides an easy to use API for this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the documentation for scikit-learn's PCA [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html): create a PCA object with `n_components=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the same documentation find the function to fit the PCA transform to the provided embedding vectors. This step essentially. For the curious, this step essentially finds the 2 dimensions (since we specified `n_components=2` that explain the most variance of the dataset, in other words the two dimensions that are most representative of the deviations of any one sample to another. So these 2 dimensions are the most important and therefore the best to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_to_fit = valid_word_embeddings.values()\n",
    "pca.fit(list(vectors_to_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we want to visualize our embeddings in these new PCA dimensions, so using the same documentation from above fill out the missing spots in the code below to transform the embeddings into the pca dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def get_pca_words(words):\n",
    "    embeddings = [valid_word_embeddings[word] for word in words]\n",
    "    pcas = [pca.transform(embedding.reshape(1, -1)) for embedding in embeddings]\n",
    "    return pcas\n",
    "\n",
    "def plot_pca_words(words, scale=1):\n",
    "    pcas = get_pca_words(words)\n",
    "    zeros = [0 for _ in pcas]\n",
    "    x_start = zeros\n",
    "    y_start = zeros\n",
    "    xs = [p[0, 0] for p in pcas]\n",
    "    ys = [p[0, 1] for p in pcas]\n",
    "    plt.quiver(x_start, y_start, xs, ys, scale=scale)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now using the above functions we can plot the corresponding pca vectors of any words we like. Below are some good examples of pairs of words that are similar within the movie review context and their corresponding vectors are also similar. **This is a good sign.** This means the embedding we have learned is likely doing something somewhat reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_words(['film', 'entertainment'], scale=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_words(['man', 'woman'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_words(['good', 'bad', 'horrible', 'great'], scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now find 2 more pairs of words that are similar in PCA'd embedding space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_words(['brilliant', 'good'], scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca_words(['annoying', 'worst'], scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given that the task we learned these embeddings for was sentiment classification, the embeddings are typically more meaningful for adjectives. Write a sentence or two about why you think this last statement makes sense intuitively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#TODO see above\n",
    "\n",
    "> It makes sense that the most meaningful embeddings are the adjectives because those are what the network should \n",
    "be looking out for to analyze sentiment. It is analyzing moview reviews and therefore the most meaningful and expressive words are going to be the adjectives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now just for fun we can write a function that gives us the 10 closest words to a provided word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_to_angle(word):\n",
    "    p = pca.transform(valid_word_embeddings[word].reshape(-1, 1))\n",
    "    return np.arctan(p[0, 1] / p[0, 0])\n",
    "valid_word_angles = [word_to_angle(word) for word in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_closest_n(value, n):\n",
    "    indices = np.argsort(np.abs(np.array(valid_word_angles) - value))\n",
    "    return [(valid_words[ind], valid_word_angles[ind]) for ind in indices[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_n(word_to_angle('terrible'), 10) # TRY IT YOURSELF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
