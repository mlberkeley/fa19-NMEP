{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy and pandas and matplotlib (as plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose:\n",
    "Classify MNSIT using SVCs (SVM classifiers) and kernels. MNIST is a dataset of handwritten digits; the task is to classify the digits as 0-9. The images themselves are 28 x 28 pixels large. \n",
    "\n",
    "Note: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set-up \n",
    "\n",
    "Import data and import train_test_split from sklearn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-799a158addf3>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-799a158addf3>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    data =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "# fetch \"MNIST original\" \n",
    "data = \n",
    "\n",
    "# determine X and y\n",
    "X = \n",
    "y = \n",
    "\n",
    "# print the shape of X and y\n",
    "\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Use train_test_split. Keep test at 25%.\n",
    "\n",
    "X_train, X_test, y_train, y_test = ...\n",
    "\n",
    "\n",
    "# The SVM algorithm runs in O(n^2) time, where n is the number of training points\n",
    "# To prevent the algorithm from taking forever, take only the first 10000 data points\n",
    "# from the training set and the first 2000 data points from the test set\n",
    "\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use 1's and 7's for our classification problem. The following code block should filter out only the 1's and 7's:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new test and train sets with only 1's and 7's\n",
    "\n",
    "train_mask = ((y_train == 1).astype(int) + (y_train == 7).astype(int)).astype(bool)\n",
    "test_mask = ((y_test == 1).astype(int) + (y_test == 7).astype(int)).astype(bool)\n",
    "\n",
    "X_test = X_test[test_mask, :]\n",
    "y_test = y_test[test_mask]\n",
    "X_train = X_train[train_mask, :]\n",
    "y_train = y_train[train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to visualize the dataset\n",
    "# Feel free to change the index\n",
    "#                  v\n",
    "plt.imshow(X_train[0].reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA from sklearn\n",
    "We will use Principal Component Analysis (PCA) to manipulate the data to make it more usable for SVC. The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set by projecting the data on to a space while still retaining as much variance in the data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-6b316a2eb63a>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-6b316a2eb63a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    pca =\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# import PCA\n",
    "\n",
    "# There are a total of 28 * 28 features (one per pixel)\n",
    "# Let's project this down to 2 features using pca (2 features so we can plot out the data in 2-d)\n",
    "pca = \n",
    "\n",
    "# Use pca to transform X_train, X_test\n",
    "X_train_pca = \n",
    "X_test_pca = \n",
    "\n",
    "#print the shape of X_train_pca \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What change do you notice between our old training data and our new one?\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC and Kernels\n",
    "\n",
    "Now we will experiment with support vector classifiers and kernels. We will need LinearSVC, SVC, and accuracy_score.\n",
    "\n",
    "SVMs are really interesting because they have something called the dual formulation, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment.\n",
    "\n",
    "A transformation that lifts the data into a higher-dimensional space is called a kernel. A poly- nomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSVC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-656ced6be746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fit the LinearSVC on X_train_pca and y_train and then print train accuracy and test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlsvc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train acc: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearSVC' is not defined"
     ]
    }
   ],
   "source": [
    "# import SVC, LinearSVC, accuracy_score\n",
    "\n",
    "# fit the LinearSVC on X_train_pca and y_train and then print train accuracy and test accuracy\n",
    "# CODE HERE\n",
    "lsvc = LinearSVC(...)\n",
    "\n",
    "print('train acc: ', )\n",
    "print('test acc: ', )\n",
    "        \n",
    "# use SVC with an RBF kernel. Fit this model on X_train_pca and y_train and print accuracy metrics as before\n",
    "# CODE HERE\n",
    "print('train acc: ', ) \n",
    "print('test acc: ', )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "Now plot out all the data points in the test set. Ones should be colored red and sevens should be colored blue. We have already provided the code to plot the decision boundary. The plot is a reault of using PCA on a 784 dimensional data.\n",
    "\n",
    "Hint: `plt.scatter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "# Code to plot the decision boundary of a linear svm\n",
    "\n",
    "weights = lsvc.coef_[0]\n",
    "x = np.linspace(-500,500,100)\n",
    "y = x / weights[1] * -weights[0]\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort!\n",
    "\n",
    "Now we're going to do a kind of hack. We've trained a linearSVM (SVC) on a binary classification problem. But what if we wanted something more regression-like? Say we wanted to score each datapoint on how \"one-y\" or how \"seven-y\" it looked. How would we do that? Check out the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC).\n",
    "\n",
    "In the block below, create a list of scores for each datapoint in `X_test_pca`. Then sort `X_test` using the scores from `X_test_pca` (we're using `X_test` instead of `X_test_pca` because we want to plot the images). The block after contains code to plot out the sorted images. You should see 1's gradually turn in to 7's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "scores = ### scores of the data points\n",
    "sorted_X = ### X_test sorted based on score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to plot the images (this may take some time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "\n",
    "def plot(x):\n",
    "    plt.imshow(x.reshape(28,28))\n",
    "    plt.show()\n",
    "\n",
    "def plot_dataset(X):\n",
    "    fig = plt.figure(1, (60, 60))\n",
    "\n",
    "    fig.subplots_adjust(left=0.05, right=0.95)\n",
    "    \n",
    "    grid = AxesGrid(fig, 141,  # similar to subplot(141)\n",
    "                    nrows_ncols=(20, 10),\n",
    "                    axes_pad=0.05,\n",
    "                    label_mode=\"1\",\n",
    "                    )\n",
    "    \n",
    "    for i in range(200):\n",
    "        grid[i].imshow(X[i].reshape(28,28))\n",
    "\n",
    "# We're assuming sorted_X has ~200 datapoints in it\n",
    "# This may take a long time to run\n",
    "plot_dataset(sorted_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "1) What is a kernel and why is it important?\n",
    "\n",
    "2) Can we kernelize all types of data? Why or why not?\n",
    "\n",
    "3) What are some pros/cons of kernels? (look into runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
