{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed in this notebook: `numpy` (as np), `matplotlib.pyplot` (as plt), from sklearn: `LinearRegression`, `ElasticNet`, and `mean_squared_error`. Search up documentation if you have issues with any of the scikit learn things. This first part will be a warmup with numpy, linear regression, and scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll generate some random data and find the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = 20\n",
    "\n",
    "def gen_data(n):\n",
    "    \n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        rand = abs(np.random.randn())\n",
    "        x += [rand]\n",
    "        y += [.15 * np.random.randn() + rand]\n",
    "\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()\n",
    "    \n",
    "    return (x, y)\n",
    "\n",
    "data = gen_data(num_data)\n",
    "data = [(data[0][i], data[1][i]) for i in range(num_data)]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `data` is a list of tuples of data. Perform linear regression to find the line of best fit, using only numpy. Minimize squared loss. Use matplotlib to plot your line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data.copy() #data1 is a copy of data, dont modify data since we'll use it later\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same thing, except using Scikit Learn. I recommend reading some documentation, specifically here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.copy()\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Orbits\n",
    "\n",
    "In this homework, we will use linear regression methods in order to determine the orbits of heavenly bodies.\n",
    "\n",
    "### Background\n",
    "\n",
    "In 1801 the minor planet Ceres was first observed for a period of 40 days before moving behind the sun. To predict the location of Ceres astronomers would have to solve complicated non-linear differential equations, quite a task in an era before computers or calculators. However, Carl Friedrich Gauss had another idea. By single handedly developing the theory of least squares and linear regression and applying it to the problem of finding Ceres, Gauss managed to accurately predict the location of the minor planet nearly a year after it's last sighting.\n",
    "\n",
    "In this problem we likewise attempt to predict the orbit of a \"planet\" and in the process \"derive\" the formula for an ellipse, the shape of orbits of heavenly bodies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate Data\n",
    "\n",
    "The idea here is we generate data in the shape of an ellipse. To do this we use the formula of an ellipse in polar coordinates:\n",
    "\n",
    "$$ r = \\frac{ep}{1-e \\cos(\\theta)} $$\n",
    "\n",
    "where $ e $ is the eccentricity and $ p $ is the distance from the minor axis to the directrix (read \"length\"). In addition, we add random noise to the data.\n",
    "\n",
    "We will then try to fit curves to our synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_data(e, p, o):\n",
    "    theta = np.linspace(0,2*np.pi, 200)\n",
    "\n",
    "    # Ellipse with eccentricity e\n",
    "    # Axis \"length\" p\n",
    "    # Offset by .5 angularly\n",
    "    r = e*p/(1-e*np.cos(theta - o)) \n",
    "\n",
    "    # transform to cartesian\n",
    "    x = r * np.cos(theta)\n",
    "    y = r * np.sin(theta)\n",
    "\n",
    "    # Add noise\n",
    "    x += np.random.randn(x.shape[0]) / 20\n",
    "    y += np.random.randn(y.shape[0]) / 20\n",
    "\n",
    "    # plot\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()\n",
    "\n",
    "    # saving\n",
    "    np.save('x.npy', x)\n",
    "    np.save('y.npy', y)\n",
    "\n",
    "gen_data(.5, 2, .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use sklearn's LinearRegression\n",
    "\n",
    "Try to fit a `LinearRegression` model to `x` and `y` (let $ x $ be the independent variable and $ y $ be the dependent variable). Print out the `mean_squared_error` you get and plot both `x`, `y` (scatter plot), and the predicted orbit (line plot).\n",
    "\n",
    "This is a really dumb idea, please explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Experimentation time!\n",
    "\n",
    "Try adding new features to your linear model by manipulating $ x $! For example, try adding a quadratic term, $ x^2 $ or a root term like $ \\sqrt{x} $. Print out the MSE of your model and plot both `x`, `y` (scatter plot), and the predicted orbit (line plot). This time, your model should take in an expanded set of features and predict $ y $.\n",
    "\n",
    "Hint: `np.vstack` may be useful here.\n",
    "\n",
    "This is still a really dumb idea, please explain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Plane Curves\n",
    "\n",
    "As you've probably figured out, the above two methods are pretty crap at predicting orbits. What we really need to do is predict a curve in the plane. First, let's erase some of the data so what we're doing is actually a challenge. Just run the code in the next box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a mask where x < 0 or y < 0\n",
    "def mask():\n",
    "    global x\n",
    "    global y\n",
    "    \n",
    "    mask = (x < 0) + (y < 0)\n",
    "    x = x[mask]\n",
    "    y = y[mask]\n",
    "    \n",
    "    # plot erased data\n",
    "    plt.scatter(x, y)\n",
    "    plt.show()\n",
    "\n",
    "mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most general form of a plane curve is\n",
    "\n",
    "$$ f(x,y) = 0 $$\n",
    "\n",
    "In order to simplify our lives a bit, let's restrict this to something of the form:\n",
    "\n",
    "$$ ax^2 + bxy + cy^2 + dx + ey + f = 0 $$\n",
    "\n",
    "You may recognize this as the general form of a conic! Let's take our data and try to predict the best possible coefficients here using least squares. This way, these coefficients should give the best possible approximation to the orbit. Print your predicted coefficients.\n",
    "\n",
    "Hint: Think about the features you need. (6 total)\n",
    "\n",
    "Hint: Use the normal equation instead of sklearn.\n",
    "\n",
    "Hint: This is going to fail, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reformulation\n",
    "\n",
    "The above should fail for a very trivial (pun intended) reason. The reason is that if we simply set all the coefficients to zero, we get a perfect solution! We can see this in the normal equations:\n",
    "\n",
    "$$ (A^TA)^{-1} A^T b = x $$\n",
    "\n",
    "but $ b = \\vec 0 $ in our case, thus $ x = \\vec 0 $ trivially.\n",
    "\n",
    "How do we get around this? One thing we can do is to not have $ b = \\vec 0 $. To do this, let us modify the general form of a plane curve a bit:\n",
    "\n",
    "$$ f(x,y) + 1 = 1 $$\n",
    "\n",
    "Now our restricted plane curve will be of the form\n",
    "\n",
    "$$ ax^2 + bxy + cy^2 + dx + ey + f + 1 = 1 $$\n",
    "\n",
    "Is this just an aesthetic change? or will this actually help? Code it up and find out! Plot your model using the handy dandy `plot_conic` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function should help you plot your ellipses:\n",
    "\n",
    "def plot_conic(coeff):\n",
    "    '''\n",
    "    params\n",
    "    ------\n",
    "    coeff : array[6] floats\n",
    "        Array of 6 floats, corresponding to \n",
    "        a, b, c, d, e, and f respectively\n",
    "        in the equation above\n",
    "    '''\n",
    "    xv = np.linspace(-9, 9, 400)\n",
    "    yv = np.linspace(-5, 5, 400)\n",
    "    xv, yv = np.meshgrid(xv, yv)\n",
    "\n",
    "    def axes():\n",
    "        plt.axhline(0, alpha=.1)\n",
    "        plt.axvline(0, alpha=.1)\n",
    "\n",
    "    axes()\n",
    "    plt.contour(xv, yv, xv*xv*coeff[0] + xv*yv*coeff[1] + yv*yv*coeff[2] + xv*coeff[3] + yv*coeff[4] + coeff[5], [0], colors='k')\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ridge\n",
    "\n",
    "So, reformulating the problem might have worked, but more than likely it didn't work too well. Here's some code to generate new data. Try running the above model multiple times on different data. More than likely most of them will look terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regenerate data\n",
    "gen_data(.5, 2, .5)\n",
    "mask()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that our method is too unstable. It turns out the Ridge Regression as a regularizer can reduce numerical instability and constrain under-constrained problems. Try rewriting the regression from above using ridge regression and see how well it does. Plot out the model using `plot_conic`. Compare the results with the previous method.\n",
    "\n",
    "Hint: Use the `regenerate data` block to try new data\n",
    "\n",
    "Hint: There is really only one extra term between this question and the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. \"Deriving\" an Ellipse\n",
    "\n",
    "LASSO regularization is a _sparse feature selector_ in the sense that it zeros out \"useless\" features and keeps relevant features. It's a good way to reduce the number of features you have to use. \n",
    "\n",
    "In this case we're going to pretend we don't know what form the equation of an ellipse takes. We can add random monomials to form a guess:\n",
    "\n",
    "$$ ax^2 + bxy + cy^2 + dx + ey + f + gx^3 + hy^3 + jx^2y + \\cdots + 1 = 1 $$\n",
    "\n",
    "The idea here is that if we use LASSO regression on the above equation, the terms irrelevant to an ellipse will \"zero out\" and the quadratic and lower terms won't! Try this out, and print out the coefficients. No gurantees this will works 100% :), but you should find that all coefficients greater than quadratic zero out.\n",
    "\n",
    "`Hint`: We want to keep the ridge regularization to maintain numerical stabilitiy. So we need a combined Ridge and LASSO regression. For some reason, this model is called `ElasticNet` from sklearn. Use that model.\n",
    "\n",
    "`Hint`: You might have to play around with the parameters a bit. I used these `l1_ratio=.23, alpha=.01` to produce some pretty good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate this model!\n",
    "\n",
    "Run this code block below. This code block assumes that you have an array called `coeff` which has  10 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xv = np.linspace(-9, 9, 400)\n",
    "yv = np.linspace(-5, 5, 400)\n",
    "xv, yv = np.meshgrid(xv, yv)\n",
    "\n",
    "def axes():\n",
    "    plt.axhline(0, alpha=.1)\n",
    "    plt.axvline(0, alpha=.1)\n",
    "\n",
    "axes()\n",
    "plt.contour(xv, yv, xv*xv*coeff[0] + xv*yv*coeff[1] + yv*yv*coeff[2] + xv*coeff[3] + yv*coeff[4] + coeff[5] - 1 + coeff[6]*xv*xv*xv + coeff[7]*yv*yv*yv + coeff[8]*xv*xv*yv + coeff[9]*xv*yv*yv , [0], colors='k')\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. More Ridge Regression & Vector Calculus!\n",
    "\n",
    "As demonstrated above, ridge regression can help overcome numerical instability and generalization issues that ordinary least squares (OLS) can fall short to. \n",
    "\n",
    "We mentioned during lecture that one of the caveats of OLS was the assumption that out input matrix, $X$, is full rank.  However, when the features of our data are close to collinear, $X$ might lose rank or have singular values very close to 0. This means $(X^TX)^{-1}$ will have extremely large singular values resulting in abnormally high values in the optimal $w$ solution (our parameters).\n",
    "\n",
    "However, there is a very simple solution for this!\n",
    "\n",
    "$$\\min_w \\Vert{\\bf Xw - y}\\Vert_2^2 + \\lambda\\Vert{\\bf w}\\Vert_2^2$$ \n",
    "\n",
    "By adding a penalty term with a fixed small scalar $\\lambda > 0$ (this is a hyperparameter!), we can prevent $w$ from becoming too large.\n",
    "\n",
    "-----\n",
    "\n",
    "In lecture we defined our OLS loss function to be:\n",
    "\n",
    "$$L(w) = \\Vert{\\bf Xw - y}\\Vert_2^2$$\n",
    "\n",
    "Our new loss function with the penalty term is:\n",
    "\n",
    "$$L(w) = \\Vert{\\bf Xw - y}\\Vert_2^2 + \\lambda\\Vert{\\bf w}\\Vert_2^2$$\n",
    "\n",
    "**TO DO:**  Using vector calculus, derive the optimal solution $w$ for the ridge regression loss function. (hint: calculate the gradient!)\n",
    "\n",
    "You can include this in your PDF submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
